{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "LSTMClassifier.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TelXoUvy7gx6"
      },
      "source": [
        "#Definition of models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HsJVVRUw01-T"
      },
      "source": [
        "\n",
        "# import standard PyTorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "mderPiXE01-V"
      },
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self, dimension):\n",
        "    super().__init__()\n",
        "    self.d = dimension\n",
        "    # define layers\n",
        "    self.fc1 = nn.Linear(in_features=dimension,out_features=512)\n",
        "    self.fc2 = nn.Linear(in_features=512,out_features=376)\n",
        "\n",
        "  def forward(self, t):\n",
        "        # fc 1\n",
        "        # t=t.reshape(-1,28*28)\n",
        "        t=t.reshape(-1, self.d)\n",
        "        t=self.fc1(t)\n",
        "        t=F.relu(t)\n",
        "\n",
        "        t=self.fc2(t)\n",
        "        # t=F.relu(t)\n",
        "        return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-yLyfF-i01-V"
      },
      "source": [
        "class Lstm(nn.Module):\n",
        "    def __init__(self, inputSize, batchSize, hiddenSize):\n",
        "        super().__init__()\n",
        "        self.i = inputSize\n",
        "        self.h = hiddenSize\n",
        "        self.b = batchSize\n",
        "        # define layers\n",
        "        self.lstm = nn.LSTM(input_size=self.i, hidden_size=self.h, batch_first=True)\n",
        "        self.h0 = torch.rand(1,self.b, self.h)/10\n",
        "        self.c0 = torch.rand(1,self.b, self.h)/10\n",
        "\n",
        "\n",
        "    def forward(self, t):\n",
        "        output,(hn,cn) = self.lstm(t,(self.h0,self.c0))\n",
        "        output = output[:,-1,:]\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9-A3hiIX01-V"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, data, labels, lengths):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.data = data\n",
        "        self.lengths = lengths\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # Load data and get label\n",
        "        x = self.data[index]\n",
        "        y = self.labels[index]\n",
        "        #adding length to return\n",
        "        z = self.lengths[index]\n",
        "        return x, y, z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azbn1uOo7ogp"
      },
      "source": [
        "# Data preperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcy6ce_R3yCK",
        "outputId": "168599dd-f9b2-443d-da87-6c5e5d483e9d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBYyvAYF01-W",
        "outputId": "87fde2ce-3692-4668-d995-e19f8fe99a45"
      },
      "source": [
        "\n",
        "train_df = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/VirtualPatient/train_ctx_anon.csv\", index_col=0)\n",
        "test_df = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/VirtualPatient/valid_ctx_anon.csv\", index_col=0)\n",
        "\n",
        "print(train_df.head())\n",
        "def getGlove():\n",
        "    glove = {}\n",
        "    with open(\"gdrive/My Drive/Colab Notebooks/VirtualPatient/glove.6B.100d.txt\",\"r\") as f:\n",
        "        for wv in  f:\n",
        "            splits  = wv.split()\n",
        "            word = splits[0]\n",
        "            vector = np.asarray(splits[1:],\"float32\")\n",
        "            glove[word] = vector\n",
        "    return glove\n",
        "glove = getGlove()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   convo_num  ...  label\n",
            "0          0  ...     31\n",
            "1          0  ...    270\n",
            "2          0  ...    256\n",
            "3          0  ...    177\n",
            "4          0  ...     84\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "jh_4_C5b01-W"
      },
      "source": [
        "# todo revisit data preperation\n",
        "def prepareData(batch_size, train, limit=0):\n",
        "    input = []\n",
        "    if train:\n",
        "        if limit == 0:\n",
        "            data = train_df['query']\n",
        "            labels = train_df['label']\n",
        "        else:\n",
        "            data = train_df['query'][:limit]\n",
        "            labels = train_df['label'][:limit]\n",
        "    else:\n",
        "        data = test_df['query']\n",
        "        labels = test_df['label']\n",
        "    maxlength = 0\n",
        "    for sentence in data:\n",
        "        sentenceVec = []\n",
        "        for word in word_tokenize(sentence):\n",
        "            if word in glove:\n",
        "                sentenceVec.append(glove[word])\n",
        "        if maxlength < len(sentence):\n",
        "            maxlength = len(sentence)\n",
        "        input.append(np.array(sentenceVec))\n",
        "    input1 = []\n",
        "    lengths =[]\n",
        "    for sentenceVec in input:\n",
        "        t = torch.tensor(sentenceVec)\n",
        "        sentenceLength = len(sentenceVec)\n",
        "        lengths.append(len(sentenceVec))\n",
        "        if batch_size >1 and sentenceLength < maxlength:\n",
        "            padding=torch.zeros(maxlength-sentenceLength, 100)\n",
        "            t = torch.cat((t,padding), dim=0)\n",
        "        input1.append(t)\n",
        "    dataset = MyDataset(input1, labels, lengths=lengths)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    return loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1AZ9g_iD01-W"
      },
      "source": [
        "batch_size=204\n",
        "loader = prepareData(batch_size,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGL6kgKa7txD"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cQrLPXT401-W"
      },
      "source": [
        "#Todo: is this correct\n",
        "import tqdm.notebook as tq\n",
        "def get_accuracy(model1, model2,dataloader):\n",
        "  count=0\n",
        "  correct=0\n",
        "\n",
        "  model1.eval()\n",
        "  model2.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in tq.tqdm_notebook(dataloader):\n",
        "      data = batch[0]\n",
        "      labels = batch[1]\n",
        "      preds = model1(data.float())\n",
        "      preds = model2(preds)\n",
        "      batch_correct=preds.argmax(dim=1).eq(labels).sum().item()\n",
        "      batch_count=len(batch[0])\n",
        "      count+=batch_count\n",
        "      correct+=batch_correct\n",
        "  model1.train()\n",
        "  model2.train()\n",
        "  return correct/count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1OEmMAVI01-X"
      },
      "source": [
        "import tqdm.notebook as tq\n",
        "lr=0.001\n",
        "shuffle=True\n",
        "epochs=10\n",
        "hiddenSize = 100\n",
        "\n",
        "lstm = Lstm(100, batch_size, hiddenSize)\n",
        "lstmOptimizer = optim.Adam(lstm.parameters(), lr=lr)\n",
        "network = Network(hiddenSize)\n",
        "networkOptimizer = optim.Adam(network.parameters(), lr=lr)\n",
        "# set the network to training mode\n",
        "lstm.train()\n",
        "network.train()\n",
        "for epoch in range(epochs):\n",
        "  for batch in tq.tqdm_notebook(loader):\n",
        "    loss = 0\n",
        "    data = batch[0]\n",
        "    labels = batch[1]\n",
        "    lstmOut = lstm(data.float())\n",
        "    preds= network(lstmOut)\n",
        "    loss = F.cross_entropy(preds, labels)\n",
        "    lstmOptimizer.zero_grad()\n",
        "    networkOptimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    lstmOptimizer.step()\n",
        "    networkOptimizer.step()\n",
        "    \n",
        "  print('Epoch {0}: train set accuracy {1}'.format(epoch,get_accuracy(lstm,network,loader)))\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size)\n",
        "# print('Epoch {0}: test set accuracy {1}'.format(epoch,get_accuracy(network,test_loader)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovkUaUu1oy0a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}